url,final_url,status,content_type,source_type,title,description,chunk_index,chunk_total,chunk_text,depth
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government,200,text/html; charset=UTF-8,html,National framework for the assurance of artificial intelligence in government | Department of Finance,"A joint approach to safe and responsible AI by the Australian, state and territory governments. The national framework for the assurance of artificial intelligence in government was agreed to and released by the Data and Digital Ministers Meeting on 21 June 2024.It establishes cornerstones and practices of AI assurance, an essential part of the broader governance of how governments use AI.The practices demonstrate how governments can practically apply Australia’s AI Ethics Principles to their assurance of AI.The cornerstones are 5 mechanisms that governments should consider to ensure effective application of the ethics principles.Use the below links to explore the DDMM statement, the framework introduction, cornerstones, practices of AI assurance and further resources.",0,1,"National framework for the assurance of artificial intelligence in government
Statement from Data and Digital Ministers
Introduction
Cornerstones of AI assurance
Implementing Australia’s AI Ethics Principles in practice
Resources
Download a PDF version
A joint approach to safe and responsible AI by the Australian, state and territory governments.
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
The national framework for the assurance of artificial intelligence in government was agreed to and released by the Data and Digital Ministers Meeting on 21 June 2024.
It establishes cornerstones and practices of AI assurance, an essential part of the broader governance of how governments use AI.
The practices demonstrate how governments can practically apply Australia’s AI Ethics Principles to their assurance of AI.
The cornerstones are 5 mechanisms that governments should consider to ensure effective application of the ethics principles.
Use the below links to explore the DDMM statement, the framework introduction, cornerstones, practices of AI assurance and further resources.
National framework for the assurance of artificial intelligence in government [PDF 2.99MB]
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.",0
https://www.finance.gov.au/sites/default/files/2024-06/National-framework-for-the-assurance-of-AI-in-government.pdf,https://www.finance.gov.au/sites/default/files/2024-06/National-framework-for-the-assurance-of-AI-in-government.pdf,200,binary,binary,,,,,,1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",0,14,"Implementing Australia’s AI Ethics Principles in government
1. Human, societal and environmental wellbeing
2. Human-centred values
In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence
3. Fairness
In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper
4. Privacy protection and security
In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy Obligations
5. Reliability and safety
6. Transparency and explainability
In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping Policy
7. Contestability
In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice Guide
8. Accountability
On this page...
1.1 Document intentions
1.2 Consult with stakeholders
1.3 Assess impact
2.1 Comply with rights protections
2.2 Incorporate diverse perspectives
2.3 Ensure digital inclusion
3.1 Define fairness in context
3.2 Comply with anti-discrimination obligations
3.3 Ensure quality of data and design
4.1 Comply with privacy obligations
4.2 Minimise and protect personal information
4.3 Secure systems and data
5.1 Use appropriate datasets
5.2 Conduct pilot studies
5.3 Test and verify
5.4 Monitor and evaluate
5.5 Be prepared to disengage
6.1 Disclose the use of AI
6.2 Maintain reliable data and information assets
6.3 Provide clear explanations
6.4 Support and enable",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",1,14,"est and verify
5.4 Monitor and evaluate
5.5 Be prepared to disengage
6.1 Disclose the use of AI
6.2 Maintain reliable data and information assets
6.3 Provide clear explanations
6.4 Support and enable frontline staff
7.1 Understand legal obligations
7.2 Communicate rights and protections clearly
8.1 Establish clear roles and responsibilities
8.2 Train staff and embed capability
8.3 Embed a positive risk culture
8.4 Avoid overreliance
Home
Statement from Data and Digital Ministers
Introduction
Cornerstones of AI assurance
Resources
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
The following practices are mapped to Australia’s 8 AI Ethics Principle s, demonstrating how governments can practically apply them to their assurance of AI.
Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.
These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:
Throughout their lifecycle, AI systems should benefit individuals,",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",2,14,"y drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:
Throughout their lifecycle, AI systems should benefit individuals, society and the environment.
Governments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.
Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.
Governments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.
Seek input from stakeholders early to allow for the early identification and mitigation of risks.
Governments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.
Methods such as algorithmic and stakeholder impact assessments may assist.
AI systems should respect human rights, diversity and the autonomy of individuals.
Governments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:
Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",3,14,"include those protected under:
Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.
Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.
Governments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.
This may include representation of:
Governments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.
Consider assistive technologies to support people who live with disability.
The CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.
The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.
Explore the guidelines on the CSIRO",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",4,14,"AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.
Explore the guidelines on the CSIRO website.
AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.
Governments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.
Governments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:
Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.
Governments should ensure high-quality data and algorithmic design.
Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.
This technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.
It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",5,14,"lores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.
This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.
AI systems should respect and uphold privacy rights of individuals and ensure the protection of data.
Governments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.
This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.
‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.
Governments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.
Consider if similar outcomes can be achieved with privacy enhancing technologies.",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",6,14,"llection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.
Consider if similar outcomes can be achieved with privacy enhancing technologies.
Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.
Sensitive information should always be managed with caution.
Governments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.
Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.
Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.
Governments should consider relevant security guidance and strategies including:
Published in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.
It covers the collection, use, handling and governance of personal information within this context.
Organisations should conduct a privacy impact assessment when designing or implementing AI systems to",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",7,14,"covers the collection, use, handling and governance of personal information within this context.
Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.
Throughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.
Governments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.
Governments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.
Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.
Governments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.
Governments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.
This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",8,14,"eration is safe, reliable and aligned to ethics principles.
This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.
Governments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.
This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.
There should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.
Governments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.
Governments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.
Governments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:
When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",9,14,"his includes:
When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.
When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.
Governments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.
Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.
Released in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.
This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.
When an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.
Governments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",10,14,"o allow people to challenge the use or outcomes of the AI system.
Governments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.
This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.
Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.
Governments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.
This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.
Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.
Released in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.
It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.
It also provides practical tools for agencies, including a checklist designed to assist",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",11,14,"untability, and monitoring and evaluation of automated decision-making systems including those that contain AI.
It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.
Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.
Those responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.
Governments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:
the role of senior leadership and area-specific responsibilities
security, data governance, privacy and other obligations
integration with existing governance and risk management frameworks.
Governments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.
Governments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.
This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",12,14,"intrinsic part of everyday practice.
This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.
Governments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.
Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.
NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)
Adoption of Artificial Intelligence in the Public Sector (DTA 2023)
Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)
Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)
Responsible AI Pattern Catalogue (CSIRO 2023)
How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)
legislation at all levels of government
Australia’s international human rights obligations
the Australian and state constitutions
interpretation of common law.
people living with disability
multi-cultural communities
religious communities
people from different socio-economic",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/implementing-australias-ai-ethics-principles-government,200,text/html; charset=UTF-8,html,Implementing Australia’s AI Ethics Principles in government | Department of Finance,"The following practices are mapped to Australia’s 8 AI Ethics Principles, demonstrating how governments can practically apply them to their assurance of AI.Their application may differ according to jurisdictional specific governance and assurance protocols. Similarly, different use cases present different risks with some requiring a higher standard of assurance than others. Therefore, not all AI use cases will require the detailed application of all available practices to be considered safe and responsible.These practices were developed by drawing extensively from the existing practices of the Australian, state and territory governments, as well as these publications:NSW Artificial Intelligence Assurance Framework (Digital NSW 2022)Adoption of Artificial Intelligence in the Public Sector (DTA 2023)Safe and responsible AI in Australia consultation: Australian Government’s  interim response (DISR 2024)Implementing Australia’s AI Ethics Principles (Gradient Institute and CSIRO 2023)Responsible AI Pattern Catalogue (CSIRO 2023)How might artificial intelligence affect the trustworthiness of public service  delivery? (PM&C 2023)1. Human, societal and environmental wellbeing Throughout their lifecycle, AI systems should benefit individuals, society and the environment.1.1 Document intentionsGovernments should define and document the purpose and objectives of a use case and the outcomes expected for people, society and the environment.Document risks, consider whether the use of AI is preferable, whether there is a clear public benefit and what non-AI alternatives are available. Existing frameworks or policies for benefits realisation may assist.1.2 Consult with stakeholdersGovernments should identify and consult with stakeholders, including subject matter and legal experts, and impacted groups and their representatives.Seek input from stakeholders early to allow for the early identification and mitigation of risks.1.3 Assess impactGovernments should assess the likely impacts of an AI use case on people, communities, societal and environmental wellbeing to determine if benefits outweigh risks and manage said impacts appropriately.Methods such as algorithmic and stakeholder impact assessments may assist.2. Human-centred valuesAI systems should respect human rights, diversity and the autonomy of individuals.2.1 Comply with rights protectionsGovernments will ensure their use of AI complies with legal protections for human rights. This may include those protected under:legislation at all levels of governmentAustralia’s international human rights obligationsthe Australian and state constitutionsinterpretation of common law.Any use will also align with related obligations, policies and guidelines for the public sector, workplace health and safety, human rights, and diversity and inclusion.Human rights impact assessments may assist to identify, assess and mitigate human rights risks. Where necessary seek advice from subject matter experts.2.2 Incorporate diverse perspectivesGovernments should involve people with different lived experiences, including marginalisation, throughout the lifecycles of a use case to gather informed perspectives, remove preconceptions and avoid overlooking important considerations.This may include representation of:people living with disabilitymulti-cultural communitiesreligious communitiespeople from different socio-economic backgroundsdiverse genders and sexualitiesAboriginal and Torres Strait Islander people.2.3 Ensure digital inclusionGovernments should align to digital service and inclusion standards, and account for the needs, context and experience of individual users across an AI use case’s lifecycle.Consider assistive technologies to support people who live with disability.In focus: The CSIRO’s Guidelines for Diversity and Inclusion in Artificial IntelligenceThe CSIRO’s Guidelines for Diversity and Inclusion in Artificial Intelligence (Zowghi D and da Rimini F 2023) address the evolving and holistic nature of AI technologies, the importance of diversity and inclusion consideration in the development and deployment of AI, and the potential consequences of neglecting it.The guidelines emphasise the importance of a socio-technical perspective on diversity and inclusion in AI, highlighting the necessity of involving relevant stakeholders with diverse attributes, examining cultural dynamics and norms, and evaluating societal impacts.Explore the guidelines on the CSIRO website.3. Fairness AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.3.1 Define fairness in contextGovernments should consider the expected benefits and potential impacts of using AI, as well as vulnerabilities of impacted groups, to determine ‘fairness’ in a use case’s context.3.2 Comply with anti-discrimination obligationsGovernments will ensure their use of AI complies with relevant anti-discrimination legislation, policies and guidelines for protected attributes. These may include:agedisabilityracereligionsexintersex statusgender identitysexual orientation.Well trained and supported staff should be able to identify, report and resolve biased AI outputs. Where necessary, seek advice from subject matter experts.3.3 Ensure quality of data and designGovernments should ensure high-quality data and algorithmic design.Audits of AI inputs and outputs for unfair biases, data quality statements and other data governance and management practices may assist to understand and mitigate bias in AI systems.In focus: the Australian Human Rights Commission’s Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical PaperThis technical paper is a collaborative partnership between the Australian Human Rights Commission, Gradient Institute, Consumer Policy Research Centre, CHOICE and CSIRO’s Data61.It explores how the problem of algorithmic bias can arise in decision making that uses artificial intelligence and how this problem can produce unfair, and potentially unlawful, decisions as it may lead to a person being unfairly treated or even suffering unlawful discrimination based on characteristics such as race, age, sex or disability. It demonstrates how the risk of algorithmic bias can be identified and steps that can be taken to address or mitigate this problem.This paper forms part of a AHRC’s Human Rights and Technology Project. You can read the technical paper on the AHRC website.4. Privacy protection and securityAI systems should respect and uphold privacy rights of individuals and ensure the protection of data.4.1 Comply with privacy obligationsGovernments will ensure their use of AI complies with legislation, policy and guidelines that govern consent, collection, storage, use, disclosure and retention of personal information.This may include informing people when their personal information is being collected for an AI system or when personal information is used for a secondary purpose such as AI system training.‘Privacy by design’ principles and privacy impact assessments may assist to identify, assess and mitigate privacy risks. Where necessary, seek advice from subject matter experts.4.2 Minimise and protect personal informationGovernments should assess whether the collection, use and disclosure of personal information is necessary, reasonable and proportionate for each AI use case.Consider if similar outcomes can be achieved with privacy enhancing technologies.Synthetic data, data anonymisation and deidentification, encryption, secure aggregation and other measures may assist to reduce privacy risks.Sensitive information should always be managed with caution.4.3 Secure systems and dataGovernments should ensure each use case complies with security and data protection legislation, policies and guidelines, including through an AI system’s supply chains.Security considerations should be consistent with the cyber security strategies and polices of impacted jurisdictions.Access to systems, applications and data repositories should be limited to authorised staff as required by their duties. Where necessary, seek advice from subject matter experts.Governments should consider relevant security guidance and strategies including:2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)Hosting Certification Framework (Home Affairs n.d.)Engaging with Artificial Intelligence (ASD 2024)Deploying AI Systems Securely (ASD 2024)Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)In focus: Office of the Victorian Information Commissioner’s Artificial Intelligence – Understanding Privacy ObligationsPublished in April 2021, the Office of the Victorian Information Commissioner’s Artificial  Intelligence – Understanding Privacy Obligations (OVIC 2021) provides guidance to assist Victorian Public Service organisations consider their privacy obligations when using or considering the use of personal information in AI systems or applications.It covers the collection, use, handling and governance of personal information within this context.Organisations should conduct a privacy impact assessment when designing or implementing AI systems to help identify potential privacy risks associated with the collection and use of personal information in the AI system.5. Reliability and safetyThroughout their lifecycle, AI systems should reliably operate in accordance with their intended purpose.5.1 Use appropriate datasetsGovernments should ensure that, wherever practical, AI systems are trained and validated on accurate, representative, authenticated and reliable datasets that are suitable for the specific use case.5.2 Conduct pilot studiesGovernments should evaluate AI systems in small-scale pilot environments to identify and mitigate problems and iterate and scale the solution.Consider the trade-offs between governance and effectiveness: a highly controlled environment may not accurately reflect the full risk and opportunity landscape, while a less controlled environment may pose governance challenges.5.3 Test and verifyGovernments should test and verify the performance of AI systems. Red teaming, conformity assessments, reinforcement from human feedback, metrics and performance testing, and other methods may assist.5.4 Monitor and evaluateGovernments should ensure their use of AI is continuously monitored and evaluated to ensure its operation is safe, reliable and aligned to ethics principles.This should encompass an AI system’s performance, its use by people, and impacts on people, society and the environment, including feedback from those impacted by AI-influenced outcomes.5.5 Be prepared to disengageGovernments should be prepared to quickly and safely disengage an AI system when an unresolvable problem is identified.This could include a data breach, unauthorised access or system compromise. Consider such scenarios in business continuity, data breach and security response plans.6. Transparency and explainabilityThere should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.6.1 Disclose the use of AIGovernments should ensure their use of AI is disclosed to users or people who may be impacted by it. Governments should maintain a register of when it uses AI, its purpose, intended uses, and limitations.6.2 Maintain reliable data and information assetsGovernments should comply with legislation, policies and standards for maintaining reliable records of decisions, testing, and the information and data assets used in an AI system. This will enable internal and external scrutiny, continuity of knowledge and accountability.6.3 Provide clear explanationsGovernments should provide clear, simple explanations for how an AI system reaches an outcome. This includes:inputs and variables and how these have influenced the reliability of the systemthe results of testing including technical and human validationthe implementation of human oversight.When explainability is limited, governments should weigh the benefits of AI use against explainability limitations. Where a decision is made to proceed with AI use, document reasons and apply heightened levels of oversight and control.When an AI system influences or is used as part of administrative decision making, decisions should be explainable, and humans accountable.6.4 Support and enable frontline staffGovernments should ensure staff at frontline agencies are well-trained and supported to clearly explain AI-influenced outcomes to users and people.Consider the importance of human-to-human relationships for a range of people, including vulnerable people or groups, people facing complex needs and those uncomfortable with government’s use of AI.In focus: Public Record Office Victoria’s AI Technologies and Recordkeeping PolicyReleased in March 2024, Victoria’s Artificial Intelligence (AI) Technologies and Recordkeeping Policy (PROV 2024) was designed to address transparency and accountability concerns in relation to AI implementation and use and to enable explainable AI use.This includes the production of full and accurate records/data, as well as the appropriate management of those records/data in accordance with the PROV Recordkeeping Standards framework.7. ContestabilityWhen an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.7.1 Understand legal obligationsGovernments will ensure their use of AI in administrative decision-making complies with law, policy and guidelines that regulate such processes.This includes principles of legality, fairness, rationality and transparency, and access to reviews, dispute resolutions and investigations.Where necessary, governments should seek legal advice as to their legal obligations and proposed use of AI.7.2 Communicate rights and protections clearlyGovernments should clearly communicate the rights and protections of those impacted by each AI use case and create an avenue to voice concerns and objections and seek recourse and redress.This includes clearly communicating the channels and processes to challenge the use or outcomes of an AI system.Feedback and response mechanisms should be clear and transparent, ensure timely human review and exist across the use case’s lifecycles.In focus: the Commonwealth Ombudsman’s Automated Decision-making Better Practice GuideReleased in March 2020, the Automated Decision-making Better Practice Guide [PDF 571KB] (Commonwealth Ombudsman 2020) recognises the significant role automation plays in administrative decision-making. The key message of the guide is that people must be at the centre of service delivery.It provides specific guidance on administrative law, privacy, governance and design, transparency and accountability, and monitoring and evaluation of automated decision-making systems including those that contain AI.It also provides practical tools for agencies, including a checklist designed to assist managers and project officers during the design and implementation of new automated systems, and ongoing assurance processes for once a system is operational.Similarly, the NSW Ombudsman has released guidance on automated decision making in the public sector.8. AccountabilityThose responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.8.1 Establish clear roles and responsibilitiesGovernments should ensure their use of AI is overseen by clearly identified roles and lines of accountability. Governments should consider:the role of senior leadership and area-specific responsibilitiessecurity, data governance, privacy and other obligationsintegration with existing governance and risk management frameworks.8.2 Train staff and embed capabilityGovernments should establish policies, procedures, and training to ensure all staff understand their duties and responsibilities, understand system limitations and implement AI assurance practices.8.3 Embed a positive risk cultureGovernments should ensure a positive risk culture, promoting open, proactive AI risk management as an intrinsic part of everyday practice.This fosters open discussion of uncertainties and opportunities, encourages staff to express their concerns and maintains processes to escalate to the appropriate accountable parties.8.4 Avoid overrelianceGovernments remain responsible for all outputs generated by AI systems and must ensure incorrect outputs are flagged and addressed.Governments should therefore consider the level of reliance on their use of AI and its potential risk and accountability challenges. Overreliance can lead to the acceptance of incorrect or biased outputs, and risks to business continuity.",13,14,"ts obligations
the Australian and state constitutions
interpretation of common law.
people living with disability
multi-cultural communities
religious communities
people from different socio-economic backgrounds
diverse genders and sexualities
Aboriginal and Torres Strait Islander people.
age
disability
race
religion
sex
intersex status
gender identity
sexual orientation.
2023-2030 Australian Cyber Security Strategy (Home Affairs 2023)
Hosting Certification Framework (Home Affairs n.d.)
Engaging with Artificial Intelligence (ASD 2024)
Deploying AI Systems Securely (ASD 2024)
Countering the Insider Threat: A guide for Australian Government (Attorney- General’s Department 2023)
inputs and variables and how these have influenced the reliability of the system
the results of testing including technical and human validation
the implementation of human oversight.",1
https://www.finance.gov.au/,https://www.finance.gov.au/,200,text/html; charset=UTF-8,html,Home | Department of Finance,© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.,0,1,"In focus
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,200,text/html; charset=UTF-8,html,Resources | Department of Finance,"ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.——(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024.——(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. ——(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024.——(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024.——(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024.Attorney-General’s Department (n.d.) Australia’s anti-discrimination law, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Human rights protections, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Public sector guidance sheets, Attorney-General’s Department website, accessed 25 March 2024.——(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General’s Department, Australian Government, accessed 25 March 2024.Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024.——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper, AHRC, Australian Government, accessed 25 March 2024.Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024.——(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024.——(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024.——(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024.  ——(2019) Artificial Intelligence: Australia’s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024.Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024.——(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024.——(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024.Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024.DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response, DISR, Australian Government, accessed 22 March 2024. ——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024.——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023, DISR, Australian Government, accessed 22 March 2024.——(2019) Australia’s AI Ethics Principles, DISR website, accessed 22 March 2024.——(2019) Australia’s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024.DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024.——(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024.Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024.Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024.——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024.——(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024.ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment, ISO, accessed 25 March 2024.——(2022) Artificial intelligence, ISO website, accessed 25 March 2024.NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024.OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024.——(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024.——(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024.OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024.——(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024.——(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024.OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence – Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence and Privacy – Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024.PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024.Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024.——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024.Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024.Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024.Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney.",0,7,"Resources
Home
Statement from Data and Digital Ministers
Introduction
Cornerstones of AI assurance
Implementing Australia’s AI Ethics Principles in government
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.
——(n.d.) Legacy ICT management , ASD website, accessed 22 April 2024.
——(2024) Deploying AI Systems Securely , ASD, Australian Government, accessed 25 March 2024.
——(2024) Engaging with Artificial Intelligence , ASD, Australian Government, accessed 25 March 2024.
——(2023) Guidelines for secure AI system development , ASD, Australian Government, accessed 25 March 2024.
——(2021) Identifying Cyber Supply Chain Risks , ASD, Australian Government, accessed 25 March 2024.
Attorney-General’s Department (n.d.) Australia’s anti-discrimination law , Attorney-General’s Department website, accessed 25 March 2024.
——(n.d.) Human rights protections , Attorney-General’s Department website, accessed 25 March 2024.
——(n.d.) Public sector guidance sheets , Attorney-General’s Department website, accessed 25 March 2024.
——(2023) Countering the Insider Threat: A guide for Australian Government , Attorney- General’s Department, Australian Government, accessed 25 March 2024.
Australian Government (2024)",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,200,text/html; charset=UTF-8,html,Resources | Department of Finance,"ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.——(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024.——(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. ——(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024.——(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024.——(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024.Attorney-General’s Department (n.d.) Australia’s anti-discrimination law, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Human rights protections, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Public sector guidance sheets, Attorney-General’s Department website, accessed 25 March 2024.——(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General’s Department, Australian Government, accessed 25 March 2024.Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024.——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper, AHRC, Australian Government, accessed 25 March 2024.Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024.——(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024.——(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024.——(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024.  ——(2019) Artificial Intelligence: Australia’s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024.Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024.——(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024.——(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024.Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024.DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response, DISR, Australian Government, accessed 22 March 2024. ——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024.——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023, DISR, Australian Government, accessed 22 March 2024.——(2019) Australia’s AI Ethics Principles, DISR website, accessed 22 March 2024.——(2019) Australia’s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024.DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024.——(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024.Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024.Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024.——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024.——(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024.ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment, ISO, accessed 25 March 2024.——(2022) Artificial intelligence, ISO website, accessed 25 March 2024.NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024.OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024.——(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024.——(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024.OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024.——(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024.——(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024.OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence – Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence and Privacy – Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024.PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024.Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024.——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024.Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024.Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024.Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney.",1,7,"25 March 2024.
——(2023) Countering the Insider Threat: A guide for Australian Government , Attorney- General’s Department, Australian Government, accessed 25 March 2024.
Australian Government (2024) Framework for Governance of Indigenous Data , National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.
Australian Human Rights Commission (n.d.) Human Rights and Technology Project , AHRC website, accessed 25 March 2024.
——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper , AHRC, Australian Government, accessed 25 March 2024.
Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.
CSIRO (n.d.) National Artificial Intelligence Centre , CSIRO website, accessed 25 March 2024.
——(n.d.) Responsible AI Network resources , CSIRO website, accessed 25 March 2024.
——(2023) Diversity and Inclusion in Artificial Intelligence , CSIRO website, accessed 25 March 2024.
——(2023) Responsible AI Pattern Catalogue , CSIRO, Australian Government, accessed 25 March 2024.
——(2019) Artificial Intelligence: Australia’s Ethics Framework , CSIRO, Australian Government, accessed 25 March 2024.
Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service , NSW Government, accessed 25 March 2024.
——(2022) Artificial Intelligence (AI) , Department of Customer Service",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,200,text/html; charset=UTF-8,html,Resources | Department of Finance,"ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.——(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024.——(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. ——(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024.——(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024.——(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024.Attorney-General’s Department (n.d.) Australia’s anti-discrimination law, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Human rights protections, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Public sector guidance sheets, Attorney-General’s Department website, accessed 25 March 2024.——(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General’s Department, Australian Government, accessed 25 March 2024.Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024.——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper, AHRC, Australian Government, accessed 25 March 2024.Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024.——(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024.——(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024.——(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024.  ——(2019) Artificial Intelligence: Australia’s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024.Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024.——(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024.——(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024.Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024.DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response, DISR, Australian Government, accessed 22 March 2024. ——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024.——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023, DISR, Australian Government, accessed 22 March 2024.——(2019) Australia’s AI Ethics Principles, DISR website, accessed 22 March 2024.——(2019) Australia’s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024.DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024.——(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024.Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024.Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024.——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024.——(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024.ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment, ISO, accessed 25 March 2024.——(2022) Artificial intelligence, ISO website, accessed 25 March 2024.NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024.OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024.——(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024.——(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024.OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024.——(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024.——(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024.OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence – Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence and Privacy – Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024.PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024.Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024.——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024.Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024.Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024.Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney.",2,7,"Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service , NSW Government, accessed 25 March 2024.
——(2022) Artificial Intelligence (AI) , Department of Customer Service website, accessed 25 March 2024.
——(2022) NSW Artificial Intelligence Assurance Framework , Department of Customer Service, NSW Government, accessed 22 March 2024.
Department of Finance (2023) Risk Management Toolkit , Finance website, accessed 25 March 2024.
DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response , DISR, Australian Government, accessed 22 March 2024.
——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024 , DISR, Australian Government, accessed 27 May 2024.
——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023 , DISR, Australian Government, accessed 22 March 2024.
——(2019) Australia’s AI Ethics Principles , DISR website, accessed 22 March 2024.
——(2019) Australia’s Artificial Intelligence Ethics Framework , DISR website, accessed 22 March 2024.
DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector , Australian Government Architecture website, accessed 25 March 2024.
——(2019) Interim guidance on government use of public generative AI tools , Australian Government Architecture website, accessed 25 March 2024.
Government of South Australia (2023)",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,200,text/html; charset=UTF-8,html,Resources | Department of Finance,"ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.——(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024.——(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. ——(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024.——(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024.——(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024.Attorney-General’s Department (n.d.) Australia’s anti-discrimination law, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Human rights protections, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Public sector guidance sheets, Attorney-General’s Department website, accessed 25 March 2024.——(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General’s Department, Australian Government, accessed 25 March 2024.Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024.——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper, AHRC, Australian Government, accessed 25 March 2024.Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024.——(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024.——(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024.——(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024.  ——(2019) Artificial Intelligence: Australia’s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024.Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024.——(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024.——(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024.Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024.DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response, DISR, Australian Government, accessed 22 March 2024. ——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024.——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023, DISR, Australian Government, accessed 22 March 2024.——(2019) Australia’s AI Ethics Principles, DISR website, accessed 22 March 2024.——(2019) Australia’s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024.DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024.——(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024.Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024.Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024.——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024.——(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024.ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment, ISO, accessed 25 March 2024.——(2022) Artificial intelligence, ISO website, accessed 25 March 2024.NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024.OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024.——(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024.——(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024.OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024.——(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024.——(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024.OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence – Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence and Privacy – Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024.PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024.Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024.——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024.Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024.Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024.Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney.",3,7,"sed 25 March 2024.
——(2019) Interim guidance on government use of public generative AI tools , Australian Government Architecture website, accessed 25 March 2024.
Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities , Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.
——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au , accessed 25 March 2024.
Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems , Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.
Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au , accessed 22 April 2024.
——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au , accessed 25 March 2024.
——(2023) 2023-2030 Australian Cyber Security Strategy , Home Affairs, Australian Government, accessed 22 April 2024.
ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment , ISO, accessed 25 March 2024.
——(2022) Artificial intelligence , ISO website, accessed 25 March 2024.
NSW Ombudsman (2021) Automated decision-making in the public sector , NSW Ombudsman website, accessed 25 March 2024.
OAIC (Office of",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,200,text/html; charset=UTF-8,html,Resources | Department of Finance,"ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.——(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024.——(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. ——(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024.——(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024.——(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024.Attorney-General’s Department (n.d.) Australia’s anti-discrimination law, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Human rights protections, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Public sector guidance sheets, Attorney-General’s Department website, accessed 25 March 2024.——(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General’s Department, Australian Government, accessed 25 March 2024.Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024.——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper, AHRC, Australian Government, accessed 25 March 2024.Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024.——(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024.——(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024.——(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024.  ——(2019) Artificial Intelligence: Australia’s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024.Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024.——(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024.——(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024.Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024.DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response, DISR, Australian Government, accessed 22 March 2024. ——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024.——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023, DISR, Australian Government, accessed 22 March 2024.——(2019) Australia’s AI Ethics Principles, DISR website, accessed 22 March 2024.——(2019) Australia’s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024.DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024.——(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024.Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024.Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024.——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024.——(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024.ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment, ISO, accessed 25 March 2024.——(2022) Artificial intelligence, ISO website, accessed 25 March 2024.NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024.OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024.——(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024.——(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024.OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024.——(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024.——(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024.OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence – Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence and Privacy – Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024.PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024.Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024.——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024.Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024.Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024.Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney.",4,7,"22) Artificial intelligence , ISO website, accessed 25 March 2024.
NSW Ombudsman (2021) Automated decision-making in the public sector , NSW Ombudsman website, accessed 25 March 2024.
OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments , OAIC website, accessed 25 March 2024.
——(n.d.) State and territory privacy legislation , OAIC website, accessed 25 March 2024.
——(2018) Guide to data analytics and the Australian Privacy Principles , OAIC, Australian Government, accessed 25 March 2024.
OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system , OECD, accessed 25 March 2024.
——(2023) OECD AI Principles overview , OECD website, accessed 22 March 2024.
——(2023) Recommendation of the Council on Artificial Intelligence , OECD/LEGAL/0449, OECD, accessed 22 March 2024.
OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.
——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector , OVIC, State Government of Victoria, accessed 25 March 2024.
——(2018) Artificial Intelligence – Understanding Privacy Obligations , OVIC, State Government of Victoria, accessed 25 March 2024.
——(2018) Artificial Intelligence and Privacy – Issues and Challenges , OVIC, State Government of Victoria, accessed 25 March 2024.
PM&C",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,200,text/html; charset=UTF-8,html,Resources | Department of Finance,"ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.——(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024.——(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. ——(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024.——(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024.——(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024.Attorney-General’s Department (n.d.) Australia’s anti-discrimination law, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Human rights protections, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Public sector guidance sheets, Attorney-General’s Department website, accessed 25 March 2024.——(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General’s Department, Australian Government, accessed 25 March 2024.Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024.——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper, AHRC, Australian Government, accessed 25 March 2024.Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024.——(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024.——(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024.——(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024.  ——(2019) Artificial Intelligence: Australia’s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024.Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024.——(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024.——(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024.Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024.DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response, DISR, Australian Government, accessed 22 March 2024. ——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024.——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023, DISR, Australian Government, accessed 22 March 2024.——(2019) Australia’s AI Ethics Principles, DISR website, accessed 22 March 2024.——(2019) Australia’s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024.DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024.——(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024.Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024.Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024.——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024.——(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024.ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment, ISO, accessed 25 March 2024.——(2022) Artificial intelligence, ISO website, accessed 25 March 2024.NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024.OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024.——(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024.——(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024.OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024.——(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024.——(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024.OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence – Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence and Privacy – Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024.PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024.Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024.——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024.Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024.Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024.Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney.",5,7,"s , OVIC, State Government of Victoria, accessed 25 March 2024.
——(2018) Artificial Intelligence and Privacy – Issues and Challenges , OVIC, State Government of Victoria, accessed 25 March 2024.
PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery ?, PM&C, Australian Government, accessed 22 March 2024.
Public Record Office Victoria (2024) Artificial Intelligence , PROV website, accessed 25 March 2024.
——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy , PROV, State Government of Victoria, accessed 25 March 2024.
Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government , Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.
Queensland State Archives (2024) Artificial Intelligence and public records , Queensland State Archives website, accessed 25 March 2024.
Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources , Gradient Institute and CSIRO, accessed 25 March 2024.
Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems , Addison-Wesley, Sydney.
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land,",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/resources,200,text/html; charset=UTF-8,html,Resources | Department of Finance,"ASD (Australian Signals Directorate) (n.d.) Information Security Manual (ISM), ASD, Australian Government, accessed 25 March 2024.——(n.d.) Legacy ICT management, ASD website, accessed 22 April 2024.——(2024) Deploying AI Systems Securely, ASD, Australian Government, accessed 25 March 2024. ——(2024) Engaging with Artificial Intelligence, ASD, Australian Government, accessed 25 March 2024.——(2023) Guidelines for secure AI system development, ASD, Australian Government, accessed 25 March 2024.——(2021) Identifying Cyber Supply Chain Risks, ASD, Australian Government, accessed 25 March 2024.Attorney-General’s Department (n.d.) Australia’s anti-discrimination law, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Human rights protections, Attorney-General’s Department website, accessed 25 March 2024.——(n.d.) Public sector guidance sheets, Attorney-General’s Department website, accessed 25 March 2024.——(2023) Countering the Insider Threat: A guide for Australian Government, Attorney- General’s Department, Australian Government, accessed 25 March 2024.Australian Government (2024) Framework for Governance of Indigenous Data, National Indigenous Australians Agency, Australian Government, accessed 30 May 2024.Australian Human Rights Commission (n.d.) Human Rights and Technology Project, AHRC website, accessed 25 March 2024.——(2021) Using artificial intelligence to make decisions: Addressing the problem of algorithmic bias • Technical Paper, AHRC, Australian Government, accessed 25 March 2024.Commonwealth Ombudsman (2020) Automated Decision-making Better Practice Guide [PDF 571KB], Commonwealth Ombudsman, Australian Government, accessed 25 March 2024.CSIRO (n.d.) National Artificial Intelligence Centre, CSIRO website, accessed 25 March 2024.——(n.d.) Responsible AI Network resources, CSIRO website, accessed 25 March 2024.——(2023) Diversity and Inclusion in Artificial Intelligence, CSIRO website, accessed 25 March 2024.——(2023) Responsible AI Pattern Catalogue, CSIRO, Australian Government, accessed 25 March 2024.  ——(2019) Artificial Intelligence: Australia’s Ethics Framework, CSIRO, Australian Government, accessed 25 March 2024.Department of Customer Service (n.d.) Generative AI: basic guidance, Department of Customer Service, NSW Government, accessed 25 March 2024.——(2022) Artificial Intelligence (AI), Department of Customer Service website, accessed 25 March 2024.——(2022) NSW Artificial Intelligence Assurance Framework, Department of Customer Service, NSW Government, accessed 22 March 2024.Department of Finance (2023) Risk Management Toolkit, Finance website, accessed 25 March 2024.DISR (Department of Industry, Science and Resources) (2024) Safe and responsible AI in Australia consultation: Australian Government’s interim response, DISR, Australian Government, accessed 22 March 2024. ——(2024) The Seoul Declaration by countries attending the AI Seoul Summit, 21-22 May 2024, DISR, Australian Government, accessed 27 May 2024.——(2023) The Bletchley Declaration by Countries Attending the AI Safety Summit, 1–2 November 2023, DISR, Australian Government, accessed 22 March 2024.——(2019) Australia’s AI Ethics Principles, DISR website, accessed 22 March 2024.——(2019) Australia’s Artificial Intelligence Ethics Framework, DISR website, accessed 22 March 2024.DTA (Digital Transformation Agency) (2023) Adoption of Artificial Intelligence in the Public Sector, Australian Government Architecture website, accessed 25 March 2024.——(2019) Interim guidance on government use of public generative AI tools, Australian Government Architecture website, accessed 25 March 2024.Government of South Australia (2023) Guideline for the use of Large Language Model AI Tools and Utilities, Office of the Chief Information Officer, Department of the Premier and Cabinet, Government of South Australia, accessed 25 March 2024.——(n.d.) Online Accessibility Toolkit [website], accessibility.sa.gov.au, accessed 25 March 2024.Hiroshima AI Process (2023) Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems, Ministry of Internal Affairs and Communications, Government of Japan, accessed 25 March 2024.Home Affairs (Department of Home Affairs) (n.d.) Hosting Certification Framework [website], hostingcertification.gov.au, accessed 22 April 2024.——(n.d.) Protective Security Policy Framework (PSPF) [website], protectivesecurity.gov.au, accessed 25 March 2024.——(2023) 2023-2030 Australian Cyber Security Strategy, Home Affairs, Australian Government, accessed 22 April 2024.ISO (International Organization for Standardization) (2024) ISO/IEC DIS 42005 - Information technology — Artificial intelligence — AI system impact assessment, ISO, accessed 25 March 2024.——(2022) Artificial intelligence, ISO website, accessed 25 March 2024.NSW Ombudsman (2021) Automated decision-making in the public sector, NSW Ombudsman website, accessed 25 March 2024.OAIC (Office of the Australian Information Commissioner) (n.d.) Privacy impact assessments, OAIC website, accessed 25 March 2024.——(n.d.) State and territory privacy legislation, OAIC website, accessed 25 March 2024.——(2018) Guide to data analytics and the Australian Privacy Principles, OAIC, Australian Government, accessed 25 March 2024.OECD (Organization for Economic Cooperation and Development) (2024) Explanatory memorandum on the updated OECD definition of an AI system, OECD, accessed 25 March 2024.——(2023) OECD AI Principles overview, OECD website, accessed 22 March 2024.——(2023) Recommendation of the Council on Artificial Intelligence, OECD/LEGAL/0449, OECD, accessed 22 March 2024.OVIC (Office of the Victorian Information Commissioner) (2024) Public Statement: Use of personal information with ChatGPT, OVIC, State Government of Victoria, accessed 27 May 2024.——(2023) Public Statement: Use of Microsoft 365 Copilot in the Victorian public sector, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence – Understanding Privacy Obligations, OVIC, State Government of Victoria, accessed 25 March 2024.——(2018) Artificial Intelligence and Privacy – Issues and Challenges, OVIC, State Government of Victoria, accessed 25 March 2024.PM&C (Department of Prime Minister and Cabinet) (2023) How might artificial intelligence affect the trustworthiness of public service delivery?, PM&C, Australian Government, accessed 22 March 2024.Public Record Office Victoria (2024) Artificial Intelligence, PROV website, accessed 25 March 2024.——(2024) Artificial Intelligence (AI) Technologies and Recordkeeping Policy, PROV, State Government of Victoria, accessed 25 March 2024.Queensland Government Customer and Digital Group (2023) Use of generative AI in Queensland Government, Department of Transport and Main Roads, Queensland Government, accessed 25 March 2024.Queensland State Archives (2024) Artificial Intelligence and public records, Queensland State Archives website, accessed 25 March 2024.Reid A, O’Callaghan S and Lu Y (2023) Implementing Australia’s AI Ethics Principles: A selection of Responsible AI practices and resources, Gradient Institute and CSIRO, accessed 25 March 2024.Zowghi D and da Rimini F (2023) ‘Diversity and Inclusion in Artificial Intelligence’, in Lu Q et al. (eds) Responsible AI: Best Practices for Creating Trustworthy AI Systems, Addison-Wesley, Sydney.",6,7,"or Creating Trustworthy AI Systems , Addison-Wesley, Sydney.
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.",1
https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,200,text/html; charset=UTF-8,html,21 June 2024 | Department of Finance,"DATA AND DIGITAL MINISTERS MEETING COMMUNIQUÉ21 JUNE 2024Data and Digital Ministers convened in Darwin today. The following Ministers attended:Senator the Hon Katy Gallagher (Commonwealth)The Hon Bill Shorten MP (Commonwealth)The Hon Jihad Dib MP (New South Wales)The Hon Gabrielle Williams MP (Victoria)The Hon Bart Mellish MP (Queensland)The Hon Stephen Dawson MLC (Western Australia)Mr Chris Steel MLA (Australian Capital Territory)The Hon Selena Uibo MLA (Northern Territory)The Hon Michelle Rowland MP, Ms Dot West OAM and Dr Lyndon Ormond-Parker also attended the meeting.The Hon Stephen Mullighan MP (South Australia), the Hon Eric Abetz MP (Tasmania) and the Hon Judith Collins KC MP (New Zealand) were apologies for the meeting. These jurisdictions were represented by officials.Family and Domestic ViolenceThe Commonwealth and Victoria will lead work to improve information sharing across systems and jurisdictions about perpetrators of family and domestic violence. Ministers agreed this is a high priority. Data and Digital Ministers will provide advice to National Cabinet on how to improve information sharing on perpetrators and will work with Women and Women’s Safety Ministers on this advice.Digital InclusionMinisters noted work underway in collaboration with the First Nations Digital Inclusion Advisory Group to improve digital inclusion outcomes for First Nations people, consistent with Target 17 of the National Agreement on Closing the Gap: “By 2026, Aboriginal and Torres Strait Islander people have equal levels of digital inclusion”. Ministers welcomed achievements to date to address digital inclusion and noted more needed to be done. Ministers supported work being led by Minister Rowland to review the Universal Service Obligation, improve the National Messaging System and implement the School Student Broadband Initiative.Ministers are committed to ensuring all Australians are able to easily and safely access services using a Digital ID. Ministers launched a project to assist Australians, particularly First Nations people, to use alternative ways to identify who they are to create a Digital ID, should they choose to do so.Modernising Australia’s ID SystemMinisters agreed a Digital ID and Verifiable Credentials Strategy and common standards for verifiable credentials. The Strategy provides a framework for all governments to ensure a fast, secure, and seamless experience for Australians to access government services online. Ministers remain committed to individual control over personal information through secure, portable, convenient, voluntary, and inclusive Digital IDs and verifiable credentials.Ministers noted the enactment of the Digital ID legislation by the Commonwealth Parliament in May 2024. Building on the Digital ID and Verifiable Credentials Strategy, the Commonwealth, working with jurisdictions, will bring forward a further plan to Data and Digital Ministers by the end of 2024 to support adoption of Digital ID economy wide. It will enable more services to be accredited under the Digital ID legislation and to join the Australian Government Digital ID System.Artificial IntelligenceMinisters endorsed the national framework for the assurance of artificial intelligence in government. The framework will align governments' use of AI to ensure ethical, transparent adoption that puts the rights, well-being and interests of Australians first. All Ministers committed to flexibility, responsiveness and continued collaboration on their approach to AI assurance.National Life Events ProgramMinisters discussed opportunities to drive projects that help Australians easily find and receive information to make it easier to do what they need to, across all levels of government. To this end, Ministers agreed to commence a proof of concept life events-based checklist in the myGov app.New Zealand is not a party to this communiqué.Information on the Data and Digital Ministers Meeting, including previous meeting communiqués, can be found at www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting.",0,4,"21 June 2024
DATA AND DIGITAL MINISTERS MEETING COMMUNIQUÉ 21 JUNE 2024
Family and Domestic Violence
Digital Inclusion
Modernising Australia’s ID System
Artificial Intelligence
National Life Events Program
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
Data and Digital Ministers convened in Darwin today. The following Ministers attended:
The Hon Michelle Rowland MP, Ms Dot West OAM and Dr Lyndon Ormond-Parker also attended the meeting.
The Hon Stephen Mullighan MP (South Australia), the Hon Eric Abetz MP (Tasmania) and the Hon Judith Collins KC MP (New Zealand) were apologies for the meeting. These jurisdictions were represented by officials.
The Commonwealth and Victoria will lead work to improve information sharing across systems and jurisdictions about perpetrators of family and domestic violence. Ministers agreed this is a high priority. Data and Digital Ministers will provide advice to National Cabinet on how to improve information sharing on perpetrators and will work with Women and Women’s Safety Ministers on this advice.
Ministers noted work underway in collaboration with the First Nations Digital Inclusion Advisory Group to improve digital inclusion outcomes for First Nations people, consistent with Target 17 of the National Agreement on Closing the Gap: “By 2026, Aboriginal and Torres Strait Islander",1
https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,200,text/html; charset=UTF-8,html,21 June 2024 | Department of Finance,"DATA AND DIGITAL MINISTERS MEETING COMMUNIQUÉ21 JUNE 2024Data and Digital Ministers convened in Darwin today. The following Ministers attended:Senator the Hon Katy Gallagher (Commonwealth)The Hon Bill Shorten MP (Commonwealth)The Hon Jihad Dib MP (New South Wales)The Hon Gabrielle Williams MP (Victoria)The Hon Bart Mellish MP (Queensland)The Hon Stephen Dawson MLC (Western Australia)Mr Chris Steel MLA (Australian Capital Territory)The Hon Selena Uibo MLA (Northern Territory)The Hon Michelle Rowland MP, Ms Dot West OAM and Dr Lyndon Ormond-Parker also attended the meeting.The Hon Stephen Mullighan MP (South Australia), the Hon Eric Abetz MP (Tasmania) and the Hon Judith Collins KC MP (New Zealand) were apologies for the meeting. These jurisdictions were represented by officials.Family and Domestic ViolenceThe Commonwealth and Victoria will lead work to improve information sharing across systems and jurisdictions about perpetrators of family and domestic violence. Ministers agreed this is a high priority. Data and Digital Ministers will provide advice to National Cabinet on how to improve information sharing on perpetrators and will work with Women and Women’s Safety Ministers on this advice.Digital InclusionMinisters noted work underway in collaboration with the First Nations Digital Inclusion Advisory Group to improve digital inclusion outcomes for First Nations people, consistent with Target 17 of the National Agreement on Closing the Gap: “By 2026, Aboriginal and Torres Strait Islander people have equal levels of digital inclusion”. Ministers welcomed achievements to date to address digital inclusion and noted more needed to be done. Ministers supported work being led by Minister Rowland to review the Universal Service Obligation, improve the National Messaging System and implement the School Student Broadband Initiative.Ministers are committed to ensuring all Australians are able to easily and safely access services using a Digital ID. Ministers launched a project to assist Australians, particularly First Nations people, to use alternative ways to identify who they are to create a Digital ID, should they choose to do so.Modernising Australia’s ID SystemMinisters agreed a Digital ID and Verifiable Credentials Strategy and common standards for verifiable credentials. The Strategy provides a framework for all governments to ensure a fast, secure, and seamless experience for Australians to access government services online. Ministers remain committed to individual control over personal information through secure, portable, convenient, voluntary, and inclusive Digital IDs and verifiable credentials.Ministers noted the enactment of the Digital ID legislation by the Commonwealth Parliament in May 2024. Building on the Digital ID and Verifiable Credentials Strategy, the Commonwealth, working with jurisdictions, will bring forward a further plan to Data and Digital Ministers by the end of 2024 to support adoption of Digital ID economy wide. It will enable more services to be accredited under the Digital ID legislation and to join the Australian Government Digital ID System.Artificial IntelligenceMinisters endorsed the national framework for the assurance of artificial intelligence in government. The framework will align governments' use of AI to ensure ethical, transparent adoption that puts the rights, well-being and interests of Australians first. All Ministers committed to flexibility, responsiveness and continued collaboration on their approach to AI assurance.National Life Events ProgramMinisters discussed opportunities to drive projects that help Australians easily find and receive information to make it easier to do what they need to, across all levels of government. To this end, Ministers agreed to commence a proof of concept life events-based checklist in the myGov app.New Zealand is not a party to this communiqué.Information on the Data and Digital Ministers Meeting, including previous meeting communiqués, can be found at www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting.",1,4,"Advisory Group to improve digital inclusion outcomes for First Nations people, consistent with Target 17 of the National Agreement on Closing the Gap: “By 2026, Aboriginal and Torres Strait Islander people have equal levels of digital inclusion”. Ministers welcomed achievements to date to address digital inclusion and noted more needed to be done. Ministers supported work being led by Minister Rowland to review the Universal Service Obligation, improve the National Messaging System and implement the School Student Broadband Initiative.
Ministers are committed to ensuring all Australians are able to easily and safely access services using a Digital ID. Ministers launched a project to assist Australians, particularly First Nations people, to use alternative ways to identify who they are to create a Digital ID, should they choose to do so.
Ministers agreed a Digital ID and Verifiable Credentials Strategy and common standards for verifiable credentials. The Strategy provides a framework for all governments to ensure a fast, secure, and seamless experience for Australians to access government services online. Ministers remain committed to individual control over personal information through secure, portable, convenient, voluntary, and inclusive Digital IDs and verifiable credentials.
Ministers noted the enactment of the Digital ID legislation by the Commonwealth Parliament in May 2024. Building on the Digital ID and Verifiable Credentials Strategy, the Commonwealth, working with",1
https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,200,text/html; charset=UTF-8,html,21 June 2024 | Department of Finance,"DATA AND DIGITAL MINISTERS MEETING COMMUNIQUÉ21 JUNE 2024Data and Digital Ministers convened in Darwin today. The following Ministers attended:Senator the Hon Katy Gallagher (Commonwealth)The Hon Bill Shorten MP (Commonwealth)The Hon Jihad Dib MP (New South Wales)The Hon Gabrielle Williams MP (Victoria)The Hon Bart Mellish MP (Queensland)The Hon Stephen Dawson MLC (Western Australia)Mr Chris Steel MLA (Australian Capital Territory)The Hon Selena Uibo MLA (Northern Territory)The Hon Michelle Rowland MP, Ms Dot West OAM and Dr Lyndon Ormond-Parker also attended the meeting.The Hon Stephen Mullighan MP (South Australia), the Hon Eric Abetz MP (Tasmania) and the Hon Judith Collins KC MP (New Zealand) were apologies for the meeting. These jurisdictions were represented by officials.Family and Domestic ViolenceThe Commonwealth and Victoria will lead work to improve information sharing across systems and jurisdictions about perpetrators of family and domestic violence. Ministers agreed this is a high priority. Data and Digital Ministers will provide advice to National Cabinet on how to improve information sharing on perpetrators and will work with Women and Women’s Safety Ministers on this advice.Digital InclusionMinisters noted work underway in collaboration with the First Nations Digital Inclusion Advisory Group to improve digital inclusion outcomes for First Nations people, consistent with Target 17 of the National Agreement on Closing the Gap: “By 2026, Aboriginal and Torres Strait Islander people have equal levels of digital inclusion”. Ministers welcomed achievements to date to address digital inclusion and noted more needed to be done. Ministers supported work being led by Minister Rowland to review the Universal Service Obligation, improve the National Messaging System and implement the School Student Broadband Initiative.Ministers are committed to ensuring all Australians are able to easily and safely access services using a Digital ID. Ministers launched a project to assist Australians, particularly First Nations people, to use alternative ways to identify who they are to create a Digital ID, should they choose to do so.Modernising Australia’s ID SystemMinisters agreed a Digital ID and Verifiable Credentials Strategy and common standards for verifiable credentials. The Strategy provides a framework for all governments to ensure a fast, secure, and seamless experience for Australians to access government services online. Ministers remain committed to individual control over personal information through secure, portable, convenient, voluntary, and inclusive Digital IDs and verifiable credentials.Ministers noted the enactment of the Digital ID legislation by the Commonwealth Parliament in May 2024. Building on the Digital ID and Verifiable Credentials Strategy, the Commonwealth, working with jurisdictions, will bring forward a further plan to Data and Digital Ministers by the end of 2024 to support adoption of Digital ID economy wide. It will enable more services to be accredited under the Digital ID legislation and to join the Australian Government Digital ID System.Artificial IntelligenceMinisters endorsed the national framework for the assurance of artificial intelligence in government. The framework will align governments' use of AI to ensure ethical, transparent adoption that puts the rights, well-being and interests of Australians first. All Ministers committed to flexibility, responsiveness and continued collaboration on their approach to AI assurance.National Life Events ProgramMinisters discussed opportunities to drive projects that help Australians easily find and receive information to make it easier to do what they need to, across all levels of government. To this end, Ministers agreed to commence a proof of concept life events-based checklist in the myGov app.New Zealand is not a party to this communiqué.Information on the Data and Digital Ministers Meeting, including previous meeting communiqués, can be found at www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting.",2,4,"Ministers noted the enactment of the Digital ID legislation by the Commonwealth Parliament in May 2024. Building on the Digital ID and Verifiable Credentials Strategy, the Commonwealth, working with jurisdictions, will bring forward a further plan to Data and Digital Ministers by the end of 2024 to support adoption of Digital ID economy wide. It will enable more services to be accredited under the Digital ID legislation and to join the Australian Government Digital ID System.
Ministers endorsed the national framework for the assurance of artificial intelligence in government. The framework will align governments' use of AI to ensure ethical, transparent adoption that puts the rights, well-being and interests of Australians first. All Ministers committed to flexibility, responsiveness and continued collaboration on their approach to AI assurance.
Ministers discussed opportunities to drive projects that help Australians easily find and receive information to make it easier to do what they need to, across all levels of government. To this end, Ministers agreed to commence a proof of concept life events-based checklist in the myGov app.
New Zealand is not a party to this communiqué.
Information on the Data and Digital Ministers Meeting, including previous meeting communiqués, can be found at www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting .
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their",1
https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,https://www.finance.gov.au/publications/data-and-digital-ministers-meeting-outcomes/21-june-2024,200,text/html; charset=UTF-8,html,21 June 2024 | Department of Finance,"DATA AND DIGITAL MINISTERS MEETING COMMUNIQUÉ21 JUNE 2024Data and Digital Ministers convened in Darwin today. The following Ministers attended:Senator the Hon Katy Gallagher (Commonwealth)The Hon Bill Shorten MP (Commonwealth)The Hon Jihad Dib MP (New South Wales)The Hon Gabrielle Williams MP (Victoria)The Hon Bart Mellish MP (Queensland)The Hon Stephen Dawson MLC (Western Australia)Mr Chris Steel MLA (Australian Capital Territory)The Hon Selena Uibo MLA (Northern Territory)The Hon Michelle Rowland MP, Ms Dot West OAM and Dr Lyndon Ormond-Parker also attended the meeting.The Hon Stephen Mullighan MP (South Australia), the Hon Eric Abetz MP (Tasmania) and the Hon Judith Collins KC MP (New Zealand) were apologies for the meeting. These jurisdictions were represented by officials.Family and Domestic ViolenceThe Commonwealth and Victoria will lead work to improve information sharing across systems and jurisdictions about perpetrators of family and domestic violence. Ministers agreed this is a high priority. Data and Digital Ministers will provide advice to National Cabinet on how to improve information sharing on perpetrators and will work with Women and Women’s Safety Ministers on this advice.Digital InclusionMinisters noted work underway in collaboration with the First Nations Digital Inclusion Advisory Group to improve digital inclusion outcomes for First Nations people, consistent with Target 17 of the National Agreement on Closing the Gap: “By 2026, Aboriginal and Torres Strait Islander people have equal levels of digital inclusion”. Ministers welcomed achievements to date to address digital inclusion and noted more needed to be done. Ministers supported work being led by Minister Rowland to review the Universal Service Obligation, improve the National Messaging System and implement the School Student Broadband Initiative.Ministers are committed to ensuring all Australians are able to easily and safely access services using a Digital ID. Ministers launched a project to assist Australians, particularly First Nations people, to use alternative ways to identify who they are to create a Digital ID, should they choose to do so.Modernising Australia’s ID SystemMinisters agreed a Digital ID and Verifiable Credentials Strategy and common standards for verifiable credentials. The Strategy provides a framework for all governments to ensure a fast, secure, and seamless experience for Australians to access government services online. Ministers remain committed to individual control over personal information through secure, portable, convenient, voluntary, and inclusive Digital IDs and verifiable credentials.Ministers noted the enactment of the Digital ID legislation by the Commonwealth Parliament in May 2024. Building on the Digital ID and Verifiable Credentials Strategy, the Commonwealth, working with jurisdictions, will bring forward a further plan to Data and Digital Ministers by the end of 2024 to support adoption of Digital ID economy wide. It will enable more services to be accredited under the Digital ID legislation and to join the Australian Government Digital ID System.Artificial IntelligenceMinisters endorsed the national framework for the assurance of artificial intelligence in government. The framework will align governments' use of AI to ensure ethical, transparent adoption that puts the rights, well-being and interests of Australians first. All Ministers committed to flexibility, responsiveness and continued collaboration on their approach to AI assurance.National Life Events ProgramMinisters discussed opportunities to drive projects that help Australians easily find and receive information to make it easier to do what they need to, across all levels of government. To this end, Ministers agreed to commence a proof of concept life events-based checklist in the myGov app.New Zealand is not a party to this communiqué.Information on the Data and Digital Ministers Meeting, including previous meeting communiqués, can be found at www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting.",3,4,"be found at www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting .
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.
Senator the Hon Katy Gallagher (Commonwealth)
The Hon Bill Shorten MP (Commonwealth)
The Hon Jihad Dib MP (New South Wales)
The Hon Gabrielle Williams MP (Victoria)
The Hon Bart Mellish MP (Queensland)
The Hon Stephen Dawson MLC (Western Australia)
Mr Chris Steel MLA (Australian Capital Territory)
The Hon Selena Uibo MLA (Northern Territory)",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/statement-data-and-digital-ministers,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/statement-data-and-digital-ministers,200,text/html; charset=UTF-8,html,Statement from Data and Digital Ministers | Department of Finance,"Artificial intelligence (AI), while not new, is a transformative technology undergoing accelerated development and adoption. It presents great opportunities for all levels of government to transform public service delivery and enhance societal, economic and environmental wellbeing.However, we know there are risks with governments’ use of AI that require careful oversight, including legal, privacy, security and ethical risks such as bias and fairness. The importance of managing these risks has been outlined in the Australian Government’s interim response to the safe and responsible use of AI consultation. We recognise that public confidence and trust is essential to governments embracing the opportunities and realising the full potential of AI. To gain public confidence and trust, we commit to being exemplars in the safe and responsible use of AI. This requires a lawful, ethical approach that places the rights, wellbeing and interests of people first.This national framework for the assurance of AI in government is a key step towards gaining public confidence and trust in the safe and responsible use of AI by Australia’s governments.Based on Australia’s AI Ethics Principles, it sets foundations for a nationally consistent approach to AI assurance, providing clear expectations, as well as consistency and certainty for our partners. It will assist governments to develop, procure and deploy AI in a safe and responsible way.We recognise the scale and nature of AI developments can be uncertain. However, by embedding a principles-based approach in this national framework, we commit to flexibility, responsiveness, continuing collaboration and improvement of our AI assurance processes.Our commitment to putting the rights, wellbeing and interests of people first remains steadfast and unchanged.",0,2,"Statement from Data and Digital Ministers
Home
Introduction
Cornerstones of AI assurance
Implementing Australia’s AI Ethics Principles in government
Resources
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
Artificial intelligence (AI), while not new, is a transformative technology undergoing accelerated development and adoption.
It presents great opportunities for all levels of government to transform public service delivery and enhance societal, economic and environmental wellbeing.
However, we know there are risks with governments’ use of AI that require careful oversight, including legal, privacy, security and ethical risks such as bias and fairness. The importance of managing these risks has been outlined in the Australian Government’s interim response to the safe and responsible use of AI consultation.
We recognise that public confidence and trust is essential to governments embracing the opportunities and realising the full potential of AI. To gain public confidence and trust, we commit to being exemplars in the safe and responsible use of AI. This requires a lawful, ethical approach that places the rights, wellbeing and interests of people first.
This national framework for the assurance of AI in government is a key step towards gaining public confidence and trust in the safe and responsible use of AI by",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/statement-data-and-digital-ministers,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/statement-data-and-digital-ministers,200,text/html; charset=UTF-8,html,Statement from Data and Digital Ministers | Department of Finance,"Artificial intelligence (AI), while not new, is a transformative technology undergoing accelerated development and adoption. It presents great opportunities for all levels of government to transform public service delivery and enhance societal, economic and environmental wellbeing.However, we know there are risks with governments’ use of AI that require careful oversight, including legal, privacy, security and ethical risks such as bias and fairness. The importance of managing these risks has been outlined in the Australian Government’s interim response to the safe and responsible use of AI consultation. We recognise that public confidence and trust is essential to governments embracing the opportunities and realising the full potential of AI. To gain public confidence and trust, we commit to being exemplars in the safe and responsible use of AI. This requires a lawful, ethical approach that places the rights, wellbeing and interests of people first.This national framework for the assurance of AI in government is a key step towards gaining public confidence and trust in the safe and responsible use of AI by Australia’s governments.Based on Australia’s AI Ethics Principles, it sets foundations for a nationally consistent approach to AI assurance, providing clear expectations, as well as consistency and certainty for our partners. It will assist governments to develop, procure and deploy AI in a safe and responsible way.We recognise the scale and nature of AI developments can be uncertain. However, by embedding a principles-based approach in this national framework, we commit to flexibility, responsiveness, continuing collaboration and improvement of our AI assurance processes.Our commitment to putting the rights, wellbeing and interests of people first remains steadfast and unchanged.",1,2,"llbeing and interests of people first.
This national framework for the assurance of AI in government is a key step towards gaining public confidence and trust in the safe and responsible use of AI by Australia’s governments.
Based on Australia’s AI Ethics Principles , it sets foundations for a nationally consistent approach to AI assurance, providing clear expectations, as well as consistency and certainty for our partners. It will assist governments to develop, procure and deploy AI in a safe and responsible way.
We recognise the scale and nature of AI developments can be uncertain. However, by embedding a principles-based approach in this national framework, we commit to flexibility, responsiveness, continuing collaboration and improvement of our AI assurance processes.
Our commitment to putting the rights, wellbeing and interests of people first remains steadfast and unchanged.
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",0,9,"Cornerstones of assurance
Governance
Data governance
A risk-based approach
In focus: risk-based regulation
Standards
Procurement
In focus: responsible use of generative AI
On this page...
Home
Statement from Data and Digital Ministers
Introduction
Implementing Australia’s AI Ethics Principles in government
Resources
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).
The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.
Alignment to Australia’s AI Ethics Principles , developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.
They will help governments demonstrate and achieve:
To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.
AI governance comprises the organisational structure, policies, processes, regulation, roles,",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",1,9,"cs principles, governments should also consider the following cornerstones for their assurance practices.
AI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.
The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.
Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.
Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.
Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.
At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.
They should",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",2,9,"cy level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.
They should provide the necessary information, training and resources for staff to have the knowledge and means to:
The quality of an AI model’s output is driven by the quality of its data.
It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.
Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.
Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.
The use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.
The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",3,9,"n lower-risk settings.
The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework , help to identify, assess, document and manage these risks.
Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:
This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.
During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.
Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.
The Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.
As set out in the government’s interim response, the",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",4,9,"23 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.
As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.
This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.
Where practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.
Some current AI governance and management standards include:
Governments should regularly check the Standards Australia website for new AI related standards.
Careful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:
It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.
Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.
Due diligence in procurement",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",5,9,"s and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.
Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.
Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.
Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.
Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.
Generative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.
Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",6,9,"d primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.
Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:
Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.
This includes instructions on the use and protection of classified or sensitive information including personal information.
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.
safer, more reliable and fairer outcomes for all
reduced risk of negative impact on those affected by AI
the highest ethical standards when designing, developing and implementing AI.
align with the government’s objectives
use AI ethically and lawfully
exercise discretion and judgement in using AI outputs
identify, report and mitigate risks
consider testing, transparency and accountability requirements
support the community through changes to public service delivery
clearly explain AI-influenced outcomes.
design, data and models - a context-dependent sequence",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",7,9,"parency and accountability requirements
support the community through changes to public service delivery
clearly explain AI-influenced outcomes.
design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.
verification and validation
deployment
operation and monitoring.
AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management system
AS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk management
AS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisations
AI ethics principles
clearly established accountabilities
transparency of data
access to relevant information assets
proof of performance testing throughout an AI system’s life cycle.
Interim guidance on government use of public generative AI tools (DTA 2023)
Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)
Artificial Intelligence and public records (Queensland State Archives 2024)
Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)
Public Statement: Use of personal information with ChatGPT (OVIC 2024)
Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resources
Guideline for the use of Large Language Model AI Tools",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/cornerstones-assurance,200,text/html; charset=UTF-8,html,Cornerstones of assurance | Department of Finance,"In October 2023, the Department of Prime Minister and Cabinet (PM&C) published How artificial intelligence might affect the trustworthiness of public service delivery? (PM&C 2023).The report identified that current trust in AI is low, and developing community trust would be a key enabler of government adoption of AI technology.Alignment to Australia’s AI Ethics Principles, developed by the CSIRO’s Data61 and DISR, will ensure the trustworthy use of AI by governments in Australia. Each of its 8 ethics principles inform the assurance practices found in this framework and are also consistent with the Australian government’s broader work on safe and responsible AI.They will help governments demonstrate and achieve:safer, more reliable and fairer outcomes for allreduced risk of negative impact on those affected by AIthe highest ethical standards when designing, developing and implementing AI.To effectively apply the AI ethics principles, governments should also consider the following cornerstones for their assurance practices.GovernanceAI governance comprises the organisational structure, policies, processes, regulation, roles, responsibilities and risk management frameworks that ensures the safe and responsible use of AI in a way that is fit for the future.The use of AI presents challenges that requires a combination of technical, social and legal capabilities and expertise. These cut across core government functions such as data and technology governance, privacy, human rights, diversity and inclusion, ethics, cyber security, audit, intellectual property, risk management, digital investment and procurement.Implementation of AI should therefore be driven by business or policy areas and be supported by technologists.Existing decision-making and accountability structures should be adapted and updated to govern the use of AI. This reflects the likely impacts upon a range of government functions, allows for diverse perspectives, designates lines of responsibility and provides clear sight to agency leaders of the AI uses they are accountable for.Governance structures should be proportionate and adaptable to encourage innovation while maintaining ethical standards and protecting public interests.At the agency level, leaders should commit to the safe and responsible use of AI and develop a positive AI risk culture to make open, proactive AI risk management an intrinsic part of everyday work.They should provide the necessary information, training and resources for staff to have the knowledge and means to:align with the government’s objectivesuse AI ethically and lawfullyexercise discretion and judgement in using AI outputsidentify, report and mitigate risksconsider testing, transparency and accountability requirementssupport the community through changes to public service deliveryclearly explain AI-influenced outcomes.Data governanceThe quality of an AI model’s output is driven by the quality of its data.It’s therefore important to create, collect, manage, use and maintain datasets that are authenticated, reliable, accurate and representative, and maintain robust data governance practices that complies with relevant legislation.Data governance comprises the policies, processes, structures, roles and responsibilities to achieve this and is as important as any other governance process. It ensures responsible parties understand their legislative and administrative obligations, see the value it adds to their work and their government’s objectives.Data governance is also an exercise in risk management because it allows governments to minimise risks around the data it holds, while gaining maximum value from it.A risk-based approachThe use of AI should be assessed and managed on a case-by-case basis. This ensures safe and responsible development, procurement and deployment in high- risk settings, with minimal administrative burden in lower-risk settings.The level of risk depends on the specifics of each case, including factors such as the business domain context and data characteristics. Self-assessment models, such as the NSW Artificial Intelligence Assurance Framework, help to identify, assess, document and manage these risks.Risks should be managed throughout the AI system lifecycle, including reviews at transitions between lifecycle phases. The OECD defines the phases of an AI system as:design, data and models - a context-dependent sequence encompassing planning and design, data collection, processing and model building.verification and validationdeploymentoperation and monitoring.This AI system lifecycle may be embedded within the broader project management and procurement lifecycles, and risks may need re- evaluation where a significant change occurs at any phase.During system development governments should exercise discretion, prioritising traceability for datasets, processes, and decisions based on the potential for harm. Monitoring and feedback loops should be established to address emerging risks, unintended consequences or performance issues. Plans should be made for risks presented by obsolete and legacy AI systems.Governments should also consider oversight mechanisms for high-risk settings, including but not limited to external or internal review bodies, advisory bodies or AI risk committees, to provide consistent, expert advice and recommendations.In focus: risk-based regulationThe Australian Government’s 2023 ‘Safe and Responsible AI in Australia’ consultation found strong public support for Australia to follow a risk-based approach to regulating AI.As set out in the government’s interim response, the government is now considering options for mandatory guardrails for organisations designing, developing and deploying AI systems in high-risk settings.This work focuses on testing, transparency and accountability measures and is being informed by a temporary AI expert group.StandardsWhere practical, governments should align their approaches to relevant AI standards. Standards outline specifications, procedures, and guidelines to enable the safe, responsible, consistent, and effective implementation AI in a consistent and interoperable manner.Some current AI governance and management standards include:AS ISO/IEC 42001:2023 Information technology - Artificial intelligence - Management systemAS ISO/IEC 23894:2023 Information technology - Artificial intelligence - Guidance on risk managementAS ISO/IEC 38507:2022 Information technology - Governance of IT - Governance implications of the use of artificial intelligence by organisationsGovernments should regularly check the Standards Australia website for new AI related standards.ProcurementCareful consideration must be applied to procurement documentation and contractual agreements when procuring AI systems or products. This may require consideration of:AI ethics principlesclearly established accountabilitiestransparency of dataaccess to relevant information assetsproof of performance testing throughout an AI system’s life cycle.It is essential to remain mindful of the rapid pace of AI advancements and ensure contracts are adaptable to changes in technology.Governments should also consider internal skills development and knowledge transfer between vendors and staff to ensure sufficient understanding of a system’s operation and outputs, avoid vendor lock-in and ensure that vendors and staff fulfill their responsibilities.Due diligence in procurement plays a critical role in managing new risks, such as transparency and explainability of ‘black box’ AI systems like foundation models. AI can also amplify existing risks, such as privacy and security. Governments must evaluate whether existing standard contractual clauses adequately cover these new and amplified risks.Consideration should be made to a vendor’s capability to support the review, ongoing monitoring or evaluation of a system’s outputs in the event of an incident or a stakeholder raising concerns. This should include providing evidence and support for review mechanisms.Governments may face trade-offs between a procured component’s benefits and inherent assurance challenges, and resolutions will vary according to use case and tolerance threshold.Ultimately, procurement should prioritise alignment with ethics principles alongside delivering on a government’s desired outcomes.In focus: responsible use of generative AIGenerative AI (also known as foundational models, large language models or LLMs) has garnered wide attention since the public release of ChatGPT in November 2022.Whereas traditional AI has focused primarily on analysing data and subsequently making predictions, generative AI is able to create content across a wide range of mediums, including text, images, music and programming code, based on instructions or prompts provided by a user and informed by large datasets.Recognising the potential and risk of generative AI, governments across Australia have released guidance for its use in the public service, including:Interim guidance on government use of public generative AI tools (DTA 2023)Use of generative AI in Queensland Government (Department of Transport and Main Roads, Queensland Government 2023)Artificial Intelligence and public records (Queensland State Archives 2024)Public statement: Use of Microsoft Copilot for 365  in the Victorian public sector (OVIC 2023)Public Statement: Use of personal information with ChatGPT (OVIC 2024)Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resourcesGuideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).Common across government guidance is focus on human oversight and human accountability for the use of content produced using generative AI to ensure compliance with policies, legal obligations and ethical principles.This includes instructions on the use and protection of classified or sensitive information including personal information.",8,9,"4)
Generative AI: basic guidance (Department of Customer Service, NSW Government n.d.) and accompanying strategy, policy and practical resources
Guideline for the use of Large Language Model AI Tools and Utilities (Department of Premier and Cabinet, Government of South Australia 2023).",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,200,text/html; charset=UTF-8,html,Introduction | Department of Finance,"The national framework for the assurance of AI in government provides for a nationally consistent approach for the assurance of artificial intelligence use in government. Figure 1: A diagram shows the relationship between Australia’s AI Ethics Principles, the National framework for the assurance of AI government and individual frameworks of the Australian, state and territory governments.Based on Australia’s AI Ethics Principles (DISR 2019), and consistent with broader work on safe and responsible AI, the framework establishes cornerstones and practices of AI assurance. Instead of focusing on technical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational context.Assurance is an essential part of the broader governance of how governments use AI, including its development, procurement and deployment. This enables governments to: understand the expected benefits of AI identify risks and apply mitigations ensure lawful use understand if AI is operating as expected demonstrate, through evidence, that the use of AI is safe and responsible. As the Australian, state and territory governments continue to use AI, they will likely develop new or improved assurance practices based on their unique successes, vulnerabilities and impacts. These learnings will be shared and incorporated into future iterations of this framework.  Complementary initiativesThe national framework for the assurance of AI in government complements local and global initiatives on the safe and responsible use of AI, both by governments and in wider economies. The Australian, state and territory governments will consider these and other initiatives as they develop their unique assurance approaches. Australia’s AI Ethics Framework First published in 2019 and developed by the CSIRO’s Data61 and the Department of Industry, Science and Resources (DISR). Australia’ AI Ethics Framework (DISR 2019) defines the ethics principles which inform the practices found in this national assurance framework. Explore the ethics framework on the DISR website. Safe and responsible AI in Australia Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can continue largely unimpeded.As set out in the government’s interim response to the consultation, this work will ensure AI is used safely and responsibly across the wider economy. A crucial element of this agenda is the role of government as an exemplar in the safe and responsible use of AI.Read the Australian Government’s interim response on the DISR website.NSW Artificial Intelligence Assurance FrameworkWhen published in 2022, the NSW Government became the world’s first to mandate an assurance framework for the use of AI systems.The NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) assists project teams using AI to comprehensively analyse and document their projects’ AI specific risks. It also assists teams to implement risk mitigation strategies and establish clear governance and accountability measures.Access the NSW Artificial Intelligence Assurance Framework on the digital.nsw website. OECD principles for responsible stewardship of trustworthy AI Committed to by the Australian Government when first published by the Organisation for Economic Cooperation and Development in 2019. These principles aim to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values.Read the recommendation containing the principles on the OECD website. Bletchley Declaration on AI safetyAgreed to by Australia, alongside another 27 countries and the European Union, at the UK Government’s 2023 AI Safety Summit. The declaration affirms that AI should be designed, developed, deployed, and used in a manner that is safe, human-centric, trustworthy and responsible.Read the text of the Bletchley Declaration as hosted on the DISR website.Seoul Declaration for safe, innovative and inclusive AIOn 21 May 2024, the Australian Government agreed to 3 outcomes at the AI Seoul Summit, South Korea:Declaration for safe, innovative and inclusive AIStatement of Intent toward International Cooperation on AI Safety ScienceMinisterial Statement for advancing AI safety, innovation and inclusivity.The agreements build on the Bletchley Declaration, confirming a shared understanding of opportunities and risks, and committing nations to deeper international cooperation and dialogue.Read the text of the Seoul Declaration as hosted on the DISR website.What is an AI system?In November 2023, OECD member countries approved this revised definition of an AI system: ‘A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.’To avoid definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when:the team identifies that the project, product or service uses AIa vendor describes its product or service as using AIusers, the public or other stakeholders believe the project, product or service uses AI.",0,5,"Introduction
Complementary initiatives
What is an AI system?
Australia’s AI Ethics Framework
Safe and responsible AI in Australia
NSW Artificial Intelligence Assurance Framework
OECD principles for responsible stewardship of trustworthy AI
Bletchley Declaration on AI safety
Seoul Declaration for safe, innovative and inclusive AI
Home
Statement from Data and Digital Ministers
Cornerstones of AI assurance
Implementing Australia’s AI Ethics Principles in government
Resources
Search Finance.gov.au
© Department of Finance This content is only accurate as at the date of printing or download. Refer to Home | Department of Finance to ensure you are viewing the latest version.
31/08/2025
The national framework for the assurance of AI in government provides for a nationally consistent approach for the assurance of artificial intelligence use in government.
Figure 1 : A diagram shows the relationship between Australia’s AI Ethics Principles, the National framework for the assurance of AI government and individual frameworks of the Australian, state and territory governments.
Based on Australia’s AI Ethics Principles (DISR 2019), and consistent with broader work on safe and responsible AI, the framework establishes cornerstones and practices of AI assurance. Instead of focusing on technical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,200,text/html; charset=UTF-8,html,Introduction | Department of Finance,"The national framework for the assurance of AI in government provides for a nationally consistent approach for the assurance of artificial intelligence use in government. Figure 1: A diagram shows the relationship between Australia’s AI Ethics Principles, the National framework for the assurance of AI government and individual frameworks of the Australian, state and territory governments.Based on Australia’s AI Ethics Principles (DISR 2019), and consistent with broader work on safe and responsible AI, the framework establishes cornerstones and practices of AI assurance. Instead of focusing on technical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational context.Assurance is an essential part of the broader governance of how governments use AI, including its development, procurement and deployment. This enables governments to: understand the expected benefits of AI identify risks and apply mitigations ensure lawful use understand if AI is operating as expected demonstrate, through evidence, that the use of AI is safe and responsible. As the Australian, state and territory governments continue to use AI, they will likely develop new or improved assurance practices based on their unique successes, vulnerabilities and impacts. These learnings will be shared and incorporated into future iterations of this framework.  Complementary initiativesThe national framework for the assurance of AI in government complements local and global initiatives on the safe and responsible use of AI, both by governments and in wider economies. The Australian, state and territory governments will consider these and other initiatives as they develop their unique assurance approaches. Australia’s AI Ethics Framework First published in 2019 and developed by the CSIRO’s Data61 and the Department of Industry, Science and Resources (DISR). Australia’ AI Ethics Framework (DISR 2019) defines the ethics principles which inform the practices found in this national assurance framework. Explore the ethics framework on the DISR website. Safe and responsible AI in Australia Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can continue largely unimpeded.As set out in the government’s interim response to the consultation, this work will ensure AI is used safely and responsibly across the wider economy. A crucial element of this agenda is the role of government as an exemplar in the safe and responsible use of AI.Read the Australian Government’s interim response on the DISR website.NSW Artificial Intelligence Assurance FrameworkWhen published in 2022, the NSW Government became the world’s first to mandate an assurance framework for the use of AI systems.The NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) assists project teams using AI to comprehensively analyse and document their projects’ AI specific risks. It also assists teams to implement risk mitigation strategies and establish clear governance and accountability measures.Access the NSW Artificial Intelligence Assurance Framework on the digital.nsw website. OECD principles for responsible stewardship of trustworthy AI Committed to by the Australian Government when first published by the Organisation for Economic Cooperation and Development in 2019. These principles aim to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values.Read the recommendation containing the principles on the OECD website. Bletchley Declaration on AI safetyAgreed to by Australia, alongside another 27 countries and the European Union, at the UK Government’s 2023 AI Safety Summit. The declaration affirms that AI should be designed, developed, deployed, and used in a manner that is safe, human-centric, trustworthy and responsible.Read the text of the Bletchley Declaration as hosted on the DISR website.Seoul Declaration for safe, innovative and inclusive AIOn 21 May 2024, the Australian Government agreed to 3 outcomes at the AI Seoul Summit, South Korea:Declaration for safe, innovative and inclusive AIStatement of Intent toward International Cooperation on AI Safety ScienceMinisterial Statement for advancing AI safety, innovation and inclusivity.The agreements build on the Bletchley Declaration, confirming a shared understanding of opportunities and risks, and committing nations to deeper international cooperation and dialogue.Read the text of the Seoul Declaration as hosted on the DISR website.What is an AI system?In November 2023, OECD member countries approved this revised definition of an AI system: ‘A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.’To avoid definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when:the team identifies that the project, product or service uses AIa vendor describes its product or service as using AIusers, the public or other stakeholders believe the project, product or service uses AI.",1,5,"ical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational context.
Assurance is an essential part of the broader governance of how governments use AI, including its development, procurement and deployment. This enables governments to:
As the Australian, state and territory governments continue to use AI, they will likely develop new or improved assurance practices based on their unique successes, vulnerabilities and impacts.
These learnings will be shared and incorporated into future iterations of this framework.
The national framework for the assurance of AI in government complements local and global initiatives on the safe and responsible use of AI, both by governments and in wider economies. The Australian, state and territory governments will consider these and other initiatives as they develop their unique assurance approaches.
First published in 2019 and developed by the CSIRO’s Data61 and the Department of Industry, Science and Resources (DISR). Australia’ AI Ethics Framework (DISR 2019) defines the ethics principles which inform the practices found in this national assurance framework.
Explore the ethics framework on the DISR website.
Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,200,text/html; charset=UTF-8,html,Introduction | Department of Finance,"The national framework for the assurance of AI in government provides for a nationally consistent approach for the assurance of artificial intelligence use in government. Figure 1: A diagram shows the relationship between Australia’s AI Ethics Principles, the National framework for the assurance of AI government and individual frameworks of the Australian, state and territory governments.Based on Australia’s AI Ethics Principles (DISR 2019), and consistent with broader work on safe and responsible AI, the framework establishes cornerstones and practices of AI assurance. Instead of focusing on technical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational context.Assurance is an essential part of the broader governance of how governments use AI, including its development, procurement and deployment. This enables governments to: understand the expected benefits of AI identify risks and apply mitigations ensure lawful use understand if AI is operating as expected demonstrate, through evidence, that the use of AI is safe and responsible. As the Australian, state and territory governments continue to use AI, they will likely develop new or improved assurance practices based on their unique successes, vulnerabilities and impacts. These learnings will be shared and incorporated into future iterations of this framework.  Complementary initiativesThe national framework for the assurance of AI in government complements local and global initiatives on the safe and responsible use of AI, both by governments and in wider economies. The Australian, state and territory governments will consider these and other initiatives as they develop their unique assurance approaches. Australia’s AI Ethics Framework First published in 2019 and developed by the CSIRO’s Data61 and the Department of Industry, Science and Resources (DISR). Australia’ AI Ethics Framework (DISR 2019) defines the ethics principles which inform the practices found in this national assurance framework. Explore the ethics framework on the DISR website. Safe and responsible AI in Australia Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can continue largely unimpeded.As set out in the government’s interim response to the consultation, this work will ensure AI is used safely and responsibly across the wider economy. A crucial element of this agenda is the role of government as an exemplar in the safe and responsible use of AI.Read the Australian Government’s interim response on the DISR website.NSW Artificial Intelligence Assurance FrameworkWhen published in 2022, the NSW Government became the world’s first to mandate an assurance framework for the use of AI systems.The NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) assists project teams using AI to comprehensively analyse and document their projects’ AI specific risks. It also assists teams to implement risk mitigation strategies and establish clear governance and accountability measures.Access the NSW Artificial Intelligence Assurance Framework on the digital.nsw website. OECD principles for responsible stewardship of trustworthy AI Committed to by the Australian Government when first published by the Organisation for Economic Cooperation and Development in 2019. These principles aim to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values.Read the recommendation containing the principles on the OECD website. Bletchley Declaration on AI safetyAgreed to by Australia, alongside another 27 countries and the European Union, at the UK Government’s 2023 AI Safety Summit. The declaration affirms that AI should be designed, developed, deployed, and used in a manner that is safe, human-centric, trustworthy and responsible.Read the text of the Bletchley Declaration as hosted on the DISR website.Seoul Declaration for safe, innovative and inclusive AIOn 21 May 2024, the Australian Government agreed to 3 outcomes at the AI Seoul Summit, South Korea:Declaration for safe, innovative and inclusive AIStatement of Intent toward International Cooperation on AI Safety ScienceMinisterial Statement for advancing AI safety, innovation and inclusivity.The agreements build on the Bletchley Declaration, confirming a shared understanding of opportunities and risks, and committing nations to deeper international cooperation and dialogue.Read the text of the Seoul Declaration as hosted on the DISR website.What is an AI system?In November 2023, OECD member countries approved this revised definition of an AI system: ‘A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.’To avoid definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when:the team identifies that the project, product or service uses AIa vendor describes its product or service as using AIusers, the public or other stakeholders believe the project, product or service uses AI.",2,5,"ite.
Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can continue largely unimpeded.
As set out in the government’s interim response to the consultation, this work will ensure AI is used safely and responsibly across the wider economy. A crucial element of this agenda is the role of government as an exemplar in the safe and responsible use of AI.
Read the Australian Government’s interim response on the DISR website.
When published in 2022, the NSW Government became the world’s first to mandate an assurance framework for the use of AI systems.
The NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) assists project teams using AI to comprehensively analyse and document their projects’ AI specific risks. It also assists teams to implement risk mitigation strategies and establish clear governance and accountability measures.
Access the NSW Artificial Intelligence Assurance Framework on the digital.nsw website.
Committed to by the Australian Government when first published by the Organisation for Economic Cooperation and Development in 2019. These principles aim to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values.
Read the recommendation containing the principles on the OECD website.
Agreed to by Australia, alongside",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,200,text/html; charset=UTF-8,html,Introduction | Department of Finance,"The national framework for the assurance of AI in government provides for a nationally consistent approach for the assurance of artificial intelligence use in government. Figure 1: A diagram shows the relationship between Australia’s AI Ethics Principles, the National framework for the assurance of AI government and individual frameworks of the Australian, state and territory governments.Based on Australia’s AI Ethics Principles (DISR 2019), and consistent with broader work on safe and responsible AI, the framework establishes cornerstones and practices of AI assurance. Instead of focusing on technical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational context.Assurance is an essential part of the broader governance of how governments use AI, including its development, procurement and deployment. This enables governments to: understand the expected benefits of AI identify risks and apply mitigations ensure lawful use understand if AI is operating as expected demonstrate, through evidence, that the use of AI is safe and responsible. As the Australian, state and territory governments continue to use AI, they will likely develop new or improved assurance practices based on their unique successes, vulnerabilities and impacts. These learnings will be shared and incorporated into future iterations of this framework.  Complementary initiativesThe national framework for the assurance of AI in government complements local and global initiatives on the safe and responsible use of AI, both by governments and in wider economies. The Australian, state and territory governments will consider these and other initiatives as they develop their unique assurance approaches. Australia’s AI Ethics Framework First published in 2019 and developed by the CSIRO’s Data61 and the Department of Industry, Science and Resources (DISR). Australia’ AI Ethics Framework (DISR 2019) defines the ethics principles which inform the practices found in this national assurance framework. Explore the ethics framework on the DISR website. Safe and responsible AI in Australia Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can continue largely unimpeded.As set out in the government’s interim response to the consultation, this work will ensure AI is used safely and responsibly across the wider economy. A crucial element of this agenda is the role of government as an exemplar in the safe and responsible use of AI.Read the Australian Government’s interim response on the DISR website.NSW Artificial Intelligence Assurance FrameworkWhen published in 2022, the NSW Government became the world’s first to mandate an assurance framework for the use of AI systems.The NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) assists project teams using AI to comprehensively analyse and document their projects’ AI specific risks. It also assists teams to implement risk mitigation strategies and establish clear governance and accountability measures.Access the NSW Artificial Intelligence Assurance Framework on the digital.nsw website. OECD principles for responsible stewardship of trustworthy AI Committed to by the Australian Government when first published by the Organisation for Economic Cooperation and Development in 2019. These principles aim to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values.Read the recommendation containing the principles on the OECD website. Bletchley Declaration on AI safetyAgreed to by Australia, alongside another 27 countries and the European Union, at the UK Government’s 2023 AI Safety Summit. The declaration affirms that AI should be designed, developed, deployed, and used in a manner that is safe, human-centric, trustworthy and responsible.Read the text of the Bletchley Declaration as hosted on the DISR website.Seoul Declaration for safe, innovative and inclusive AIOn 21 May 2024, the Australian Government agreed to 3 outcomes at the AI Seoul Summit, South Korea:Declaration for safe, innovative and inclusive AIStatement of Intent toward International Cooperation on AI Safety ScienceMinisterial Statement for advancing AI safety, innovation and inclusivity.The agreements build on the Bletchley Declaration, confirming a shared understanding of opportunities and risks, and committing nations to deeper international cooperation and dialogue.Read the text of the Seoul Declaration as hosted on the DISR website.What is an AI system?In November 2023, OECD member countries approved this revised definition of an AI system: ‘A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.’To avoid definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when:the team identifies that the project, product or service uses AIa vendor describes its product or service as using AIusers, the public or other stakeholders believe the project, product or service uses AI.",3,5,"e stewardship of trustworthy AI while ensuring respect for human rights and democratic values.
Read the recommendation containing the principles on the OECD website.
Agreed to by Australia, alongside another 27 countries and the European Union, at the UK Government’s 2023 AI Safety Summit. The declaration affirms that AI should be designed, developed, deployed, and used in a manner that is safe, human-centric, trustworthy and responsible.
Read the text of the Bletchley Declaration as hosted on the DISR website.
On 21 May 2024, the Australian Government agreed to 3 outcomes at the AI Seoul Summit, South Korea:
The agreements build on the Bletchley Declaration, confirming a shared understanding of opportunities and risks, and committing nations to deeper international cooperation and dialogue.
Read the text of the Seoul Declaration as hosted on the DISR website.
In November 2023, OECD member countries approved this revised definition of an AI system:
‘A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.’
To avoid definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when:
The Department of",1
https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,https://www.finance.gov.au/government/public-data/data-and-digital-ministers-meeting/national-framework-assurance-artificial-intelligence-government/introduction,200,text/html; charset=UTF-8,html,Introduction | Department of Finance,"The national framework for the assurance of AI in government provides for a nationally consistent approach for the assurance of artificial intelligence use in government. Figure 1: A diagram shows the relationship between Australia’s AI Ethics Principles, the National framework for the assurance of AI government and individual frameworks of the Australian, state and territory governments.Based on Australia’s AI Ethics Principles (DISR 2019), and consistent with broader work on safe and responsible AI, the framework establishes cornerstones and practices of AI assurance. Instead of focusing on technical detail, the framework sets foundations across all aspects of government, with jurisdictions to develop specific policies and guidance considerate of their own legislative, policy and operational context.Assurance is an essential part of the broader governance of how governments use AI, including its development, procurement and deployment. This enables governments to: understand the expected benefits of AI identify risks and apply mitigations ensure lawful use understand if AI is operating as expected demonstrate, through evidence, that the use of AI is safe and responsible. As the Australian, state and territory governments continue to use AI, they will likely develop new or improved assurance practices based on their unique successes, vulnerabilities and impacts. These learnings will be shared and incorporated into future iterations of this framework.  Complementary initiativesThe national framework for the assurance of AI in government complements local and global initiatives on the safe and responsible use of AI, both by governments and in wider economies. The Australian, state and territory governments will consider these and other initiatives as they develop their unique assurance approaches. Australia’s AI Ethics Framework First published in 2019 and developed by the CSIRO’s Data61 and the Department of Industry, Science and Resources (DISR). Australia’ AI Ethics Framework (DISR 2019) defines the ethics principles which inform the practices found in this national assurance framework. Explore the ethics framework on the DISR website. Safe and responsible AI in Australia Following consultation initiated DISR in 2023, the Australian Government committed to ensuring the use of AI systems in high-risk settings is safe and reliable while use in low-risk settings can continue largely unimpeded.As set out in the government’s interim response to the consultation, this work will ensure AI is used safely and responsibly across the wider economy. A crucial element of this agenda is the role of government as an exemplar in the safe and responsible use of AI.Read the Australian Government’s interim response on the DISR website.NSW Artificial Intelligence Assurance FrameworkWhen published in 2022, the NSW Government became the world’s first to mandate an assurance framework for the use of AI systems.The NSW Artificial Intelligence Assurance Framework (Digital NSW 2022) assists project teams using AI to comprehensively analyse and document their projects’ AI specific risks. It also assists teams to implement risk mitigation strategies and establish clear governance and accountability measures.Access the NSW Artificial Intelligence Assurance Framework on the digital.nsw website. OECD principles for responsible stewardship of trustworthy AI Committed to by the Australian Government when first published by the Organisation for Economic Cooperation and Development in 2019. These principles aim to foster innovation and trust in AI by promoting the responsible stewardship of trustworthy AI while ensuring respect for human rights and democratic values.Read the recommendation containing the principles on the OECD website. Bletchley Declaration on AI safetyAgreed to by Australia, alongside another 27 countries and the European Union, at the UK Government’s 2023 AI Safety Summit. The declaration affirms that AI should be designed, developed, deployed, and used in a manner that is safe, human-centric, trustworthy and responsible.Read the text of the Bletchley Declaration as hosted on the DISR website.Seoul Declaration for safe, innovative and inclusive AIOn 21 May 2024, the Australian Government agreed to 3 outcomes at the AI Seoul Summit, South Korea:Declaration for safe, innovative and inclusive AIStatement of Intent toward International Cooperation on AI Safety ScienceMinisterial Statement for advancing AI safety, innovation and inclusivity.The agreements build on the Bletchley Declaration, confirming a shared understanding of opportunities and risks, and committing nations to deeper international cooperation and dialogue.Read the text of the Seoul Declaration as hosted on the DISR website.What is an AI system?In November 2023, OECD member countries approved this revised definition of an AI system: ‘A machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.’To avoid definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when:the team identifies that the project, product or service uses AIa vendor describes its product or service as using AIusers, the public or other stakeholders believe the project, product or service uses AI.",4,5,"d definitional complexities the Australian, state and territory governments should consider practical guidance for staff to identify when AI assurance processes apply, such as when:
The Department of Finance acknowledges the Traditional Owners and Custodians throughout Australia and their continuing connection to land, water and community.
understand the expected benefits of AI
identify risks and apply mitigations
ensure lawful use
understand if AI is operating as expected
demonstrate, through evidence, that the use of AI is safe and responsible.
Declaration for safe, innovative and inclusive AI
Statement of Intent toward International Cooperation on AI Safety Science
Ministerial Statement for advancing AI safety, innovation and inclusivity.
the team identifies that the project, product or service uses AI
a vendor describes its product or service as using AI
users, the public or other stakeholders believe the project, product or service uses AI.",1
